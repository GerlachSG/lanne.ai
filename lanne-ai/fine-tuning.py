import torch
import json
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
import warnings
import time
from datetime import datetime
import gc
warnings.filterwarnings('ignore')

# ========== CONFIGURA√á√ïES ==========
NOME_MODELO_BASE = "microsoft/Phi-3-mini-4k-instruct"
NOME_MODELO_FINETUNED = "lanne-ai-final"
ARQUIVO_DATASET = "dataset_prepared.jsonl"

print("\n" + "="*80)
print("                    üöÄ LANNE.AI - TREINAMENTO GENERATIVO PT-BR")
print("="*80)
print(f"üìÖ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)

# ========== CALLBACK CUSTOMIZADO PARA MONITORAMENTO ==========
class MonitorCallback(TrainerCallback):
    """Callback para mostrar detalhes durante o treino"""
    def __init__(self):
        self.start_time = time.time()
        self.step_times = []
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        """Chamado a cada logging_steps"""
        if state.global_step > 0:
            # Calcular tempo m√©dio por step
            elapsed = time.time() - self.start_time
            avg_time_per_step = elapsed / state.global_step
            remaining_steps = state.max_steps - state.global_step
            eta_seconds = remaining_steps * avg_time_per_step
            eta_minutes = eta_seconds / 60
            
            print("\n" + "-"*60)
            print(f"üìä STEP {state.global_step}/{state.max_steps}")
            print("-"*60)
            
            # Mostrar m√©tricas
            if logs:
                if 'loss' in logs:
                    print(f"   üìâ Loss de treino: {logs['loss']:.4f}")
                if 'eval_loss' in logs:
                    print(f"   üìà Loss de valida√ß√£o: {logs['eval_loss']:.4f}")
                if 'learning_rate' in logs:
                    print(f"   üéØ Learning rate: {logs['learning_rate']:.2e}")
            
            # Tempo e mem√≥ria
            print(f"   ‚è±Ô∏è  Tempo decorrido: {elapsed/60:.1f} min")
            print(f"   ‚è≥ Tempo restante estimado: {eta_minutes:.1f} min")
            
            # Mem√≥ria GPU
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1e9
                reserved = torch.cuda.memory_reserved() / 1e9
                print(f"   üéÆ GPU Mem: {allocated:.1f}GB alocado / {reserved:.1f}GB reservado")
            
            print("-"*60)
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """Chamado ao final de cada √©poca"""
        print("\n" + "üåü"*30)
        print(f"   √âPOCA {int(state.epoch)} CONCLU√çDA!")
        print("üåü"*30 + "\n")

# ========== 1. VERIFICAR AMBIENTE ==========
print("\n[CHECAGEM INICIAL] Verificando ambiente...")
print("-"*60)

# Verificar GPU
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"‚úÖ GPU detectada: {gpu_name}")
    print(f"‚úÖ VRAM total: {gpu_memory:.1f} GB")
    print(f"‚úÖ CUDA version: {torch.version.cuda}")
    
    # Limpar cache GPU
    torch.cuda.empty_cache()
    gc.collect()
    print(f"‚úÖ Cache GPU limpo")
else:
    print("‚ùå GPU n√£o detectada! O treino ser√° MUITO lento.")
    resposta = input("Continuar mesmo assim? (s/n): ")
    if resposta.lower() != 's':
        exit(1)

print("-"*60)

# ========== 2. CARREGAR E ANALISAR DATASET ==========
print("\n[PASSO 1/10] üìÇ Carregando e analisando dataset...")
print("-"*60)

try:
    # Carregar dataset
    dataset = load_dataset("json", data_files=ARQUIVO_DATASET, split="train")
    print(f"‚úÖ Dataset carregado: {len(dataset)} exemplos")
    
    # Mostrar estat√≠sticas
    print("\nüìä Estat√≠sticas do dataset:")
    
    # Amostra de perguntas
    print("\nüîç Primeiras 3 perguntas:")
    for i in range(min(3, len(dataset))):
        print(f"   {i+1}. {dataset[i]['question'][:80]}...")
    
    # Verificar tamanhos
    questions_lens = [len(ex['question']) for ex in dataset]
    answers_lens = [len(ex['answer']) for ex in dataset]
    
    print(f"\nüìè Tamanho m√©dio:")
    print(f"   ‚Ä¢ Perguntas: {sum(questions_lens)/len(questions_lens):.0f} caracteres")
    print(f"   ‚Ä¢ Respostas: {sum(answers_lens)/len(answers_lens):.0f} caracteres")
    
    # Dividir treino/valida√ß√£o
    print("\nüîÑ Dividindo dataset...")
    dataset = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = dataset["train"]
    eval_dataset = dataset["test"]
    
    print(f"‚úÖ Dataset dividido:")
    print(f"   ‚Ä¢ Treino: {len(train_dataset)} exemplos (90%)")
    print(f"   ‚Ä¢ Valida√ß√£o: {len(eval_dataset)} exemplos (10%)")
    
except Exception as e:
    print(f"‚ùå Erro ao carregar dataset: {e}")
    exit(1)

print("-"*60)
input("\n‚è∏Ô∏è  Pressione ENTER para continuar com o carregamento do modelo...")

# ========== 3. CARREGAR TOKENIZADOR ==========
print("\n[PASSO 2/10] üî§ Carregando tokenizador...")
print("-"*60)

tokenizer = AutoTokenizer.from_pretrained(
    NOME_MODELO_BASE, 
    trust_remote_code=True,
    use_fast=False
)

# Configurar tokens especiais
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print(f"‚úÖ Tokenizador carregado")
print(f"   ‚Ä¢ Vocabul√°rio: {len(tokenizer)} tokens")
print(f"   ‚Ä¢ Pad token: '{tokenizer.pad_token}'")
print(f"   ‚Ä¢ EOS token: '{tokenizer.eos_token}'")
print("-"*60)

# ========== 4. FORMATAR DATASET COM EXEMPLOS ==========
print("\n[PASSO 3/10] üé® Formatando dataset para chat...")
print("-"*60)

def formatar_exemplo(exemplo):
    """Formata com system prompt fixo em PT-BR"""
    context = exemplo.get("context", "")
    question = exemplo["question"]
    answer = exemplo["answer"]
    
    # System prompt FIXO em PT-BR
    system_prompt = """Voc√™ √© Lanne.AI, assistente especializada em Linux e Debian.
IMPORTANTE: 
- Responda SEMPRE em portugu√™s brasileiro
- Seja t√©cnica mas acess√≠vel  
- Se faltar informa√ß√£o, PERGUNTE antes de responder
- Foque em solu√ß√µes pr√°ticas e comandos verific√°veis
"""
    
    if context:
        system_prompt += f"\nContexto espec√≠fico: {context}"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
        {"role": "assistant", "content": answer}
    ]
    
    # Aplicar template do Phi-3
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )
    
    return {"text": text}

# Formatar datasets
print("üîÑ Formatando datasets...")
train_dataset = train_dataset.map(formatar_exemplo, desc="Formatando treino")
eval_dataset = eval_dataset.map(formatar_exemplo, desc="Formatando valida√ß√£o")

# Mostrar exemplo formatado
print("\nüìù EXEMPLO FORMATADO COMPLETO:")
print("="*60)
exemplo = train_dataset[0]["text"]
print(exemplo[:800])
if len(exemplo) > 800:
    print(f"... [cortado - total de {len(exemplo)} caracteres]")
print("="*60)

# Tokenizar um exemplo para ver o tamanho
tokens = tokenizer(train_dataset[0]["text"], return_tensors="pt")
print(f"\nüìä Exemplo tokenizado: {tokens['input_ids'].shape[1]} tokens")
print("-"*60)

input("\n‚è∏Ô∏è  Pressione ENTER para carregar o modelo (vai usar ~6GB de VRAM)...")

# ========== 5. CONFIGURAR QUANTIZA√á√ÉO ==========
print("\n[PASSO 4/10] ‚öôÔ∏è Configurando quantiza√ß√£o 4-bit...")
print("-"*60)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

print("‚úÖ Configura√ß√£o de quantiza√ß√£o:")
print("   ‚Ä¢ Tipo: NF4 (Normal Float 4-bit)")
print("   ‚Ä¢ Compute dtype: bfloat16")
print("   ‚Ä¢ Double quantization: Ativado")
print("   ‚Ä¢ Economia estimada: ~75% da VRAM")
print("-"*60)

# ========== 6. CARREGAR MODELO BASE ==========
print("\n[PASSO 5/10] üß† Carregando modelo Phi-3-mini...")
print("-"*60)
print("‚è≥ Isso vai levar 1-2 minutos...")
print("   Baixando ~3.8GB na primeira vez")
print("   Carregando na GPU com quantiza√ß√£o...")

inicio_carga = time.time()

model = AutoModelForCausalLM.from_pretrained(
    NOME_MODELO_BASE,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

tempo_carga = time.time() - inicio_carga
print(f"\n‚úÖ Modelo carregado em {tempo_carga:.1f} segundos!")

# Preparar para treino com quantiza√ß√£o
model = prepare_model_for_kbit_training(model)
model.config.use_cache = False

# Mostrar uso de mem√≥ria
if torch.cuda.is_available():
    memoria_usada = torch.cuda.memory_allocated() / 1e9
    print(f"üéÆ Mem√≥ria GPU usada: {memoria_usada:.2f} GB")

print("-"*60)

# ========== 7. CONFIGURAR E APLICAR LORA ==========
print("\n[PASSO 6/10] üîß Configurando LoRA (adaptadores eficientes)...")
print("-"*60)

peft_config = LoraConfig(
    r=32,  # Rank
    lora_alpha=64,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "qkv_proj",
        "o_proj", 
        "gate_up_proj",
        "down_proj"
    ]
)

print("üìã Configura√ß√£o LoRA:")
print(f"   ‚Ä¢ Rank (r): 32")
print(f"   ‚Ä¢ Alpha: 64")
print(f"   ‚Ä¢ Dropout: 0.1")
print(f"   ‚Ä¢ M√≥dulos alvo: qkv_proj, o_proj, gate_up_proj, down_proj")

# Aplicar LoRA
print("\nüîÑ Aplicando LoRA ao modelo...")
model = get_peft_model(model, peft_config)

# Mostrar par√¢metros trein√°veis
print("\nüìä PAR√ÇMETROS DO MODELO:")
print("-"*40)
model.print_trainable_parameters()
print("-"*40)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"üéØ Efici√™ncia: Treinando apenas {trainable_params/total_params*100:.2f}% dos par√¢metros!")
print("-"*60)

# ========== 8. CONFIGURAR ARGUMENTOS DE TREINAMENTO ==========
print("\n[PASSO 7/10] üìù Configurando hiperpar√¢metros...")
print("-"*60)

# Calcular steps totais
steps_per_epoch = len(train_dataset) // (2 * 4)  # batch_size * gradient_accumulation
total_steps = steps_per_epoch * 2  # 2 √©pocas

training_args = TrainingArguments(
    output_dir="./lanne_checkpoints",
    
    # Batch e gradientes
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    
    # Learning rate
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_steps=50,
    
    # √âpocas e passos
    num_train_epochs=2,
    max_steps=total_steps,
    
    # Avalia√ß√£o e salvamento
    eval_strategy="steps",
    eval_steps=max(50, steps_per_epoch//4),
    save_strategy="steps",
    save_steps=max(100, steps_per_epoch//2),
    
    # Logging detalhado
    logging_steps=10,
    logging_first_step=True,
    logging_dir="./logs",
    
    # Otimiza√ß√µes para RTX 3060
    optim="paged_adamw_8bit",
    gradient_checkpointing=True,
    max_grad_norm=0.3,
    
    # Mixed precision
    bf16=True,
    bf16_full_eval=True,
    
    # Controles
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    
    # Desabilitar relat√≥rios externos
    report_to="none",
    push_to_hub=False,
    
    # Seeds para reprodutibilidade
    seed=42,
    data_seed=42,
)

print("üìä CONFIGURA√á√ÉO DO TREINAMENTO:")
print(f"   ‚Ä¢ Batch size efetivo: {2 * 4} (2 * 4 acumula√ß√£o)")
print(f"   ‚Ä¢ Total de steps: ~{total_steps}")
print(f"   ‚Ä¢ Steps por √©poca: ~{steps_per_epoch}")
print(f"   ‚Ä¢ Learning rate: 2e-4 com cosine decay")
print(f"   ‚Ä¢ Warmup steps: 50")
print(f"   ‚Ä¢ Avalia√ß√£o a cada: {max(50, steps_per_epoch//4)} steps")
print(f"   ‚Ä¢ Checkpoint a cada: {max(100, steps_per_epoch//2)} steps")
print(f"   ‚Ä¢ Otimizador: AdamW 8-bit paginado")
print(f"   ‚Ä¢ Mixed precision: bfloat16")
print(f"   ‚Ä¢ Gradient checkpointing: Ativado (economiza VRAM)")
print("-"*60)

# ========== 9. CRIAR TRAINER ==========
print("\n[PASSO 8/10] üèãÔ∏è Preparando trainer...")
print("-"*60)

# Instanciar callback de monitoramento
monitor_callback = MonitorCallback()

# Criar trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=None,  # J√° aplicamos LoRA
    dataset_text_field="text",
    max_seq_length=1024,
    args=training_args,
    callbacks=[monitor_callback],
)

print("‚úÖ Trainer configurado e pronto!")
print("-"*60)

# ========== 10. TESTE R√ÅPIDO PR√â-TREINO ==========
print("\n[PASSO 9/10] üß™ Teste r√°pido PR√â-TREINO...")
print("-"*60)

# Fazer uma infer√™ncia r√°pida para ver como est√° antes do treino
test_prompt = "Como listar arquivos no Linux?"
messages = [
    {"role": "system", "content": "Voc√™ √© Lanne.AI, assistente Linux em PT-BR."},
    {"role": "user", "content": test_prompt}
]

input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

print(f"üîç Pergunta teste: '{test_prompt}'")
print("üí≠ Resposta ANTES do treino:")
print("-"*40)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extrair apenas a resposta
    if "assistant" in response.lower():
        response = response.split("assistant")[-1].strip()
    print(response[:200])

print("-"*60)

# ========== AVISO FINAL ANTES DO TREINO ==========
print("\n" + "üö®"*30)
print("         PRONTO PARA INICIAR O TREINAMENTO!")
print("üö®"*30)
print(f"\nüìä RESUMO FINAL:")
print(f"   ‚Ä¢ Modelo: Phi-3-mini 4k")
print(f"   ‚Ä¢ Dataset: {len(train_dataset)} exemplos de treino")
print(f"   ‚Ä¢ √âpocas: 2")
print(f"   ‚Ä¢ Tempo estimado: 60-90 minutos")
print(f"   ‚Ä¢ VRAM necess√°ria: ~8-10 GB")
print(f"   ‚Ä¢ Checkpoints salvos em: ./lanne_checkpoints")

print("\n‚ö†Ô∏è  AVISOS:")
print("   ‚Ä¢ N√ÉO feche o terminal")
print("   ‚Ä¢ N√ÉO suspenda o computador")
print("   ‚Ä¢ A GPU ficar√° em 100% de uso")
print("   ‚Ä¢ Voc√™ ver√° updates a cada 10 steps")

input("\nüöÄ Pressione ENTER para COME√áAR O TREINAMENTO...")

# ========== TREINAR! ==========
print("\n" + "="*80)
print("                        üî• INICIANDO TREINAMENTO")
print("="*80)
print(f"‚è∞ In√≠cio: {datetime.now().strftime('%H:%M:%S')}")
print("="*80 + "\n")

try:
    # Executar treinamento
    trainer.train()
    
    print("\n" + "="*80)
    print("                     ‚úÖ TREINAMENTO CONCLU√çDO!")
    print("="*80)
    
except KeyboardInterrupt:
    print("\n\n‚ö†Ô∏è Treinamento interrompido pelo usu√°rio!")
    print("Salvando checkpoint de emerg√™ncia...")
    trainer.save_model("./lanne_emergency_checkpoint")
    print("Checkpoint salvo em: ./lanne_emergency_checkpoint")
    exit(1)
    
except Exception as e:
    print(f"\n\n‚ùå Erro durante o treinamento: {e}")
    print("Salvando checkpoint de emerg√™ncia...")
    trainer.save_model("./lanne_emergency_checkpoint")
    exit(1)

# ========== 11. SALVAR MODELO FINAL ==========
print("\n[PASSO 10/10] üíæ Salvando modelo final...")
print("-"*60)

# Salvar modelo e tokenizador
trainer.save_model(NOME_MODELO_FINETUNED)
tokenizer.save_pretrained(NOME_MODELO_FINETUNED)

print(f"‚úÖ Modelo salvo em: ./{NOME_MODELO_FINETUNED}/")
print(f"‚úÖ Tokenizador salvo")
print("-"*60)

# ========== 12. TESTE P√ìS-TREINO ==========
print("\nüß™ Teste r√°pido P√ìS-TREINO...")
print("-"*60)

print(f"üîç Mesma pergunta: '{test_prompt}'")
print("üéØ Resposta DEPOIS do treino:")
print("-"*40)

# Recarregar para teste (opcional, j√° est√° em mem√≥ria)
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "assistant" in response.lower():
        response = response.split("assistant")[-1].strip()
    print(response[:300])

print("-"*60)

# ========== RELAT√ìRIO FINAL ==========
print("\n" + "="*80)
print("                        üìä RELAT√ìRIO FINAL")
print("="*80)

# Tempo total
tempo_total = time.time() - monitor_callback.start_time
print(f"\n‚è±Ô∏è  Tempo total de treino: {tempo_total/60:.1f} minutos")

# M√©tricas finais
if hasattr(trainer.state, 'log_history'):
    history = trainer.state.log_history
    final_loss = [h.get('loss') for h in history if 'loss' in h]
    if final_loss:
        print(f"üìâ Loss final de treino: {final_loss[-1]:.4f}")
    
    eval_losses = [h.get('eval_loss') for h in history if 'eval_loss' in h]
    if eval_losses:
        print(f"üìà Loss final de valida√ß√£o: {eval_losses[-1]:.4f}")
        print(f"   Melhoria: {((eval_losses[0] - eval_losses[-1])/eval_losses[0]*100):.1f}%")

# Arquivos gerados
import os
if os.path.exists(NOME_MODELO_FINETUNED):
    size = sum(os.path.getsize(os.path.join(NOME_MODELO_FINETUNED, f)) 
               for f in os.listdir(NOME_MODELO_FINETUNED)) / 1e6
    print(f"\nüíæ Tamanho do modelo salvo: {size:.1f} MB")

print("\nüéâ SUCESSO TOTAL!")
print("="*80)
print("\nüìù PR√ìXIMOS PASSOS:")
print("   1. Execute: python chat_lanne.py")
print("   2. Teste com perguntas sobre Linux")
print("   3. O modelo j√° responde em PT-BR!")
print("\n" + "="*80)